{
    "pipeline_params": {
        "method": "cacheRAG_lingua",
        "model_name": "/mnt/LLMs/Meta-Llama-3.1-8B-Instruct",
        "tokenizer_name": "/mnt/LLMs/Meta-Llama-3.1-8B-Instruct",
        "chat_template": "llama3",
        "model_max_len": 32000,
        "use_flash_attn": true,
        "truncation_mode": null,
        "batch_size": 1,
        "out_of_max_len_allowed": true,
        "rope_theta_factor": 32.0
    }
}
